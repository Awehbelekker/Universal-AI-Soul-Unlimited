name: AI Model Benchmarks

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'benchmarks/**'
      - '.github/workflows/benchmarks.yml'
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly on Sundays at 00:00 UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Model to benchmark (leave empty for all)'
        required: false
        default: ''
      num_samples:
        description: 'Number of samples per benchmark'
        required: false
        default: '50'

jobs:
  benchmark:
    name: Run AI Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    strategy:
      matrix:
        python-version: ['3.11']
      fail-fast: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests aiohttp asyncio
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama --version
      
      - name: Start Ollama service
        run: |
          ollama serve &
          sleep 5
          ollama list
      
      - name: Pull Qwen models
        run: |
          # Pull smallest model for CI/CD (faster)
          ollama pull qwen2.5:3b
          
          # Optionally pull other models if triggered manually
          if [ "${{ github.event.inputs.model_name }}" != "" ]; then
            ollama pull ${{ github.event.inputs.model_name }}
          fi
          
          ollama list
      
      - name: Run quick benchmark tests
        working-directory: benchmarks
        run: |
          # Run each benchmark individually to verify they work
          python -c "
          import asyncio
          from arc_benchmark import ARCBenchmark
          
          async def test():
              async def dummy_fn(p): return 'A'
              bench = ARCBenchmark(num_samples=5, variant='easy')
              result = await bench.run_benchmark(dummy_fn, 'test')
              print(f'ARC-Easy: {result.accuracy}% accuracy')
          
          asyncio.run(test())
          "
          
          python -c "
          import asyncio
          from truthfulqa_benchmark import TruthfulQABenchmark
          
          async def test():
              async def dummy_fn(p): return 'B'
              bench = TruthfulQABenchmark(num_samples=5)
              result = await bench.run_benchmark(dummy_fn, 'test')
              print(f'TruthfulQA: {result.accuracy}% accuracy')
          
          asyncio.run(test())
          "
          
          python -c "
          import asyncio
          from gsm8k_benchmark import GSM8KBenchmark
          
          async def test():
              async def dummy_fn(p): return '42'
              bench = GSM8KBenchmark(num_samples=5)
              result = await bench.run_benchmark(dummy_fn, 'test')
              print(f'GSM8K: {result.accuracy}% accuracy')
          
          asyncio.run(test())
          "
      
      - name: Run comprehensive benchmarks
        working-directory: benchmarks
        run: |
          NUM_SAMPLES=${{ github.event.inputs.num_samples || '20' }}
          python run_comprehensive_benchmarks.py
        env:
          OLLAMA_HOST: http://localhost:11434
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmarks/benchmark_results/*.json
          retention-days: 30
      
      - name: Generate benchmark badge
        if: github.ref == 'refs/heads/main'
        run: |
          # Extract latest accuracy from results
          LATEST_RESULT=$(ls -t benchmarks/benchmark_results/comprehensive_*.json | head -1)
          
          if [ -f "$LATEST_RESULT" ]; then
            # Simple badge generation (could use shields.io API)
            echo "Latest benchmark results available in artifacts"
          fi
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Find latest results file
            const resultsDir = 'benchmarks/benchmark_results';
            const files = fs.readdirSync(resultsDir)
              .filter(f => f.startsWith('comprehensive_'))
              .sort()
              .reverse();
            
            if (files.length > 0) {
              const resultsPath = path.join(resultsDir, files[0]);
              const results = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));
              
              let comment = '## ðŸ¤– AI Benchmark Results\n\n';
              
              for (const [modelName, modelData] of Object.entries(results.models || {})) {
                comment += `### ${modelName}\n\n`;
                comment += '| Benchmark | Accuracy | Correct/Total |\n';
                comment += '|-----------|----------|---------------|\n';
                
                for (const [benchName, benchData] of Object.entries(modelData.benchmarks || {})) {
                  comment += `| ${benchName} | ${benchData.accuracy.toFixed(1)}% | ${benchData.num_correct}/${benchData.num_questions} |\n`;
                }
                comment += '\n';
              }
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
      
      - name: Check benchmark regression
        if: github.event_name == 'pull_request'
        run: |
          # Compare with main branch benchmarks
          # This is a placeholder - implement actual comparison logic
          echo "Checking for benchmark regressions..."
          
          LATEST_RESULT=$(ls -t benchmarks/benchmark_results/comprehensive_*.json | head -1)
          
          if [ -f "$LATEST_RESULT" ]; then
            echo "âœ“ Benchmarks completed"
            
            # Could add threshold checks here
            # Example: Fail if accuracy drops > 5%
          fi

  cleanup:
    name: Cleanup old artifacts
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Delete old benchmark artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
            });
            
            // Keep only last 10 benchmark artifacts
            const benchmarkArtifacts = artifacts.data.artifacts
              .filter(a => a.name.startsWith('benchmark-results-'))
              .sort((a, b) => new Date(b.created_at) - new Date(a.created_at))
              .slice(10);
            
            for (const artifact of benchmarkArtifacts) {
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
              });
              console.log(`Deleted artifact: ${artifact.name}`);
            }
